{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from data_utils import FaceData\n",
    "import time\n",
    "import os\n",
    "\n",
    "from classifiers.segmentation_bis import SegmentationNN\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of torch.FloatTensor object at 0x7efc8e79b508>\n",
      "torch.Size([3, 200, 200])\n"
     ]
    }
   ],
   "source": [
    "train_data = FaceData(image_paths_file='LAG/train/train.txt')\n",
    "val_data = FaceData(image_paths_file='LAG/val/val.txt')\n",
    "print(train_data[0][1].size)\n",
    "print(train_data[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Train size: %i\" % len(train_data))\n",
    "print(\"Validation size: %i\" % len(val_data))\n",
    "print(\"Img size: \", train_data[0][0].size())\n",
    "print(\"Segmentation size: \", train_data[0][1].size())\n",
    "\n",
    "num_example_imgs = 9\n",
    "plt.figure(figsize=(10, 5 * num_example_imgs))\n",
    "for i, (img, target) in enumerate(train_data[:num_example_imgs]):\n",
    "    # img\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 1)\n",
    "    plt.imshow(img.numpy().reshape(200,200), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Input image\")\n",
    "    \n",
    "    # target\n",
    "    plt.subplot(num_example_imgs, 2, i * 2 + 2)\n",
    "    plt.imshow(target.numpy().reshape(200,200), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    if i == 0:\n",
    "        plt.title(\"Target image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suleman/anaconda3/lib/python3.6/site-packages/torchvision-0.2.0-py3.6.egg/torchvision/transforms/transforms.py:156: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n"
     ]
    }
   ],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Scale(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = 'LAG'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=True, num_workers=4)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAIN.\n",
      "[Iteration 1/29840] TRAIN loss: 5.649\n",
      "[Iteration 2/29840] TRAIN loss: 5.639\n",
      "[Iteration 3/29840] TRAIN loss: 5.635\n",
      "[Iteration 4/29840] TRAIN loss: 5.629\n",
      "[Iteration 5/29840] TRAIN loss: 5.632\n",
      "[Iteration 6/29840] TRAIN loss: 5.618\n",
      "[Iteration 7/29840] TRAIN loss: 5.624\n",
      "[Iteration 8/29840] TRAIN loss: 5.636\n",
      "[Iteration 9/29840] TRAIN loss: 5.600\n",
      "[Iteration 10/29840] TRAIN loss: 5.629\n",
      "[Iteration 11/29840] TRAIN loss: 5.622\n",
      "[Iteration 12/29840] TRAIN loss: 5.634\n",
      "[Iteration 13/29840] TRAIN loss: 5.614\n",
      "[Iteration 14/29840] TRAIN loss: 5.601\n",
      "[Iteration 15/29840] TRAIN loss: 5.598\n",
      "[Iteration 16/29840] TRAIN loss: 5.610\n",
      "[Iteration 17/29840] TRAIN loss: 5.589\n",
      "[Iteration 18/29840] TRAIN loss: 5.567\n",
      "[Iteration 19/29840] TRAIN loss: 5.604\n",
      "[Iteration 20/29840] TRAIN loss: 5.557\n",
      "[Iteration 21/29840] TRAIN loss: 5.572\n",
      "[Iteration 22/29840] TRAIN loss: 5.574\n",
      "[Iteration 23/29840] TRAIN loss: 5.551\n",
      "[Iteration 24/29840] TRAIN loss: 5.560\n",
      "[Iteration 25/29840] TRAIN loss: 5.562\n",
      "[Iteration 26/29840] TRAIN loss: 5.532\n",
      "[Iteration 27/29840] TRAIN loss: 5.550\n",
      "[Iteration 28/29840] TRAIN loss: 5.549\n",
      "[Iteration 29/29840] TRAIN loss: 5.545\n",
      "[Iteration 30/29840] TRAIN loss: 5.517\n",
      "[Iteration 31/29840] TRAIN loss: 5.539\n",
      "[Iteration 32/29840] TRAIN loss: 5.531\n",
      "[Iteration 33/29840] TRAIN loss: 5.535\n",
      "[Iteration 34/29840] TRAIN loss: 5.562\n",
      "[Iteration 35/29840] TRAIN loss: 5.538\n",
      "[Iteration 36/29840] TRAIN loss: 5.531\n",
      "[Iteration 37/29840] TRAIN loss: 5.554\n",
      "[Iteration 38/29840] TRAIN loss: 5.522\n",
      "[Iteration 39/29840] TRAIN loss: 5.526\n",
      "[Iteration 40/29840] TRAIN loss: 5.519\n",
      "[Iteration 41/29840] TRAIN loss: 5.514\n",
      "[Iteration 42/29840] TRAIN loss: 5.528\n",
      "[Iteration 43/29840] TRAIN loss: 5.540\n",
      "[Iteration 44/29840] TRAIN loss: 5.537\n",
      "[Iteration 45/29840] TRAIN loss: 5.517\n",
      "[Iteration 46/29840] TRAIN loss: 5.543\n",
      "[Iteration 47/29840] TRAIN loss: 5.562\n",
      "[Iteration 48/29840] TRAIN loss: 5.547\n",
      "[Iteration 49/29840] TRAIN loss: 5.517\n",
      "[Iteration 50/29840] TRAIN loss: 5.542\n",
      "[Iteration 51/29840] TRAIN loss: 5.533\n",
      "[Iteration 52/29840] TRAIN loss: 5.534\n",
      "[Iteration 53/29840] TRAIN loss: 5.510\n",
      "[Iteration 54/29840] TRAIN loss: 5.538\n",
      "[Iteration 55/29840] TRAIN loss: 5.503\n",
      "[Iteration 56/29840] TRAIN loss: 5.518\n",
      "[Iteration 57/29840] TRAIN loss: 5.522\n",
      "[Iteration 58/29840] TRAIN loss: 5.525\n",
      "[Iteration 59/29840] TRAIN loss: 5.538\n",
      "[Iteration 60/29840] TRAIN loss: 5.529\n",
      "[Iteration 61/29840] TRAIN loss: 5.509\n",
      "[Iteration 62/29840] TRAIN loss: 5.509\n",
      "[Iteration 63/29840] TRAIN loss: 5.563\n",
      "[Iteration 64/29840] TRAIN loss: 5.517\n",
      "[Iteration 65/29840] TRAIN loss: 5.515\n",
      "[Iteration 66/29840] TRAIN loss: 5.553\n",
      "[Iteration 67/29840] TRAIN loss: 5.515\n",
      "[Iteration 68/29840] TRAIN loss: 5.523\n",
      "[Iteration 69/29840] TRAIN loss: 5.517\n",
      "[Iteration 70/29840] TRAIN loss: 5.523\n",
      "[Iteration 71/29840] TRAIN loss: 5.536\n",
      "[Iteration 72/29840] TRAIN loss: 5.521\n",
      "[Iteration 73/29840] TRAIN loss: 5.532\n",
      "[Iteration 74/29840] TRAIN loss: 5.516\n",
      "[Iteration 75/29840] TRAIN loss: 5.520\n",
      "[Iteration 76/29840] TRAIN loss: 5.539\n",
      "[Iteration 77/29840] TRAIN loss: 5.504\n",
      "[Iteration 78/29840] TRAIN loss: 5.474\n",
      "[Iteration 79/29840] TRAIN loss: 5.559\n",
      "[Iteration 80/29840] TRAIN loss: 5.524\n",
      "[Iteration 81/29840] TRAIN loss: 5.478\n",
      "[Iteration 82/29840] TRAIN loss: 5.512\n",
      "[Iteration 83/29840] TRAIN loss: 5.512\n",
      "[Iteration 84/29840] TRAIN loss: 5.502\n",
      "[Iteration 85/29840] TRAIN loss: 5.509\n",
      "[Iteration 86/29840] TRAIN loss: 5.523\n",
      "[Iteration 87/29840] TRAIN loss: 5.541\n",
      "[Iteration 88/29840] TRAIN loss: 5.505\n",
      "[Iteration 89/29840] TRAIN loss: 5.510\n",
      "[Iteration 90/29840] TRAIN loss: 5.509\n",
      "[Iteration 91/29840] TRAIN loss: 5.484\n",
      "[Iteration 92/29840] TRAIN loss: 5.523\n",
      "[Iteration 93/29840] TRAIN loss: 5.462\n",
      "[Iteration 94/29840] TRAIN loss: 5.529\n",
      "[Iteration 95/29840] TRAIN loss: 5.495\n",
      "[Iteration 96/29840] TRAIN loss: 5.509\n",
      "[Iteration 97/29840] TRAIN loss: 5.504\n",
      "[Iteration 98/29840] TRAIN loss: 5.502\n",
      "[Iteration 99/29840] TRAIN loss: 5.456\n",
      "[Iteration 100/29840] TRAIN loss: 5.501\n",
      "[Iteration 101/29840] TRAIN loss: 5.518\n",
      "[Iteration 102/29840] TRAIN loss: 5.514\n",
      "[Iteration 103/29840] TRAIN loss: 5.442\n",
      "[Iteration 104/29840] TRAIN loss: 5.496\n",
      "[Iteration 105/29840] TRAIN loss: 5.495\n",
      "[Iteration 106/29840] TRAIN loss: 5.511\n",
      "[Iteration 107/29840] TRAIN loss: 5.468\n",
      "[Iteration 108/29840] TRAIN loss: 5.491\n",
      "[Iteration 109/29840] TRAIN loss: 5.477\n",
      "[Iteration 110/29840] TRAIN loss: 5.466\n",
      "[Iteration 111/29840] TRAIN loss: 5.518\n",
      "[Iteration 112/29840] TRAIN loss: 5.523\n",
      "[Iteration 113/29840] TRAIN loss: 5.491\n",
      "[Iteration 114/29840] TRAIN loss: 5.470\n",
      "[Iteration 115/29840] TRAIN loss: 5.383\n",
      "[Iteration 116/29840] TRAIN loss: 5.457\n",
      "[Iteration 117/29840] TRAIN loss: 5.496\n",
      "[Iteration 118/29840] TRAIN loss: 5.492\n",
      "[Iteration 119/29840] TRAIN loss: 5.414\n",
      "[Iteration 120/29840] TRAIN loss: 5.469\n",
      "[Iteration 121/29840] TRAIN loss: 5.437\n",
      "[Iteration 122/29840] TRAIN loss: 5.483\n",
      "[Iteration 123/29840] TRAIN loss: 5.486\n",
      "[Iteration 124/29840] TRAIN loss: 5.466\n",
      "[Iteration 125/29840] TRAIN loss: 5.505\n",
      "[Iteration 126/29840] TRAIN loss: 5.451\n",
      "[Iteration 127/29840] TRAIN loss: 5.491\n",
      "[Iteration 128/29840] TRAIN loss: 5.427\n",
      "[Iteration 129/29840] TRAIN loss: 5.515\n",
      "[Iteration 130/29840] TRAIN loss: 5.461\n",
      "[Iteration 131/29840] TRAIN loss: 5.510\n",
      "[Iteration 132/29840] TRAIN loss: 5.453\n",
      "[Iteration 133/29840] TRAIN loss: 5.471\n",
      "[Iteration 134/29840] TRAIN loss: 5.440\n",
      "[Iteration 135/29840] TRAIN loss: 5.409\n",
      "[Iteration 136/29840] TRAIN loss: 5.521\n",
      "[Iteration 137/29840] TRAIN loss: 5.518\n",
      "[Iteration 138/29840] TRAIN loss: 5.450\n",
      "[Iteration 139/29840] TRAIN loss: 5.491\n",
      "[Iteration 140/29840] TRAIN loss: 5.474\n",
      "[Iteration 141/29840] TRAIN loss: 5.452\n",
      "[Iteration 142/29840] TRAIN loss: 5.513\n",
      "[Iteration 143/29840] TRAIN loss: 5.475\n",
      "[Iteration 144/29840] TRAIN loss: 5.486\n",
      "[Iteration 145/29840] TRAIN loss: 5.457\n",
      "[Iteration 146/29840] TRAIN loss: 5.434\n",
      "[Iteration 147/29840] TRAIN loss: 5.480\n",
      "[Iteration 148/29840] TRAIN loss: 5.385\n",
      "[Iteration 149/29840] TRAIN loss: 5.428\n",
      "[Iteration 150/29840] TRAIN loss: 5.498\n",
      "[Iteration 151/29840] TRAIN loss: 5.395\n",
      "[Iteration 152/29840] TRAIN loss: 5.529\n",
      "[Iteration 153/29840] TRAIN loss: 5.512\n",
      "[Iteration 154/29840] TRAIN loss: 5.508\n",
      "[Iteration 155/29840] TRAIN loss: 5.467\n",
      "[Iteration 156/29840] TRAIN loss: 5.498\n",
      "[Iteration 157/29840] TRAIN loss: 5.393\n",
      "[Iteration 158/29840] TRAIN loss: 5.463\n",
      "[Iteration 159/29840] TRAIN loss: 5.448\n",
      "[Iteration 160/29840] TRAIN loss: 5.500\n",
      "[Iteration 161/29840] TRAIN loss: 5.478\n",
      "[Iteration 162/29840] TRAIN loss: 5.468\n",
      "[Iteration 163/29840] TRAIN loss: 5.499\n",
      "[Iteration 164/29840] TRAIN loss: 5.454\n",
      "[Iteration 165/29840] TRAIN loss: 5.485\n",
      "[Iteration 166/29840] TRAIN loss: 5.463\n",
      "[Iteration 167/29840] TRAIN loss: 5.443\n",
      "[Iteration 168/29840] TRAIN loss: 5.459\n",
      "[Iteration 169/29840] TRAIN loss: 5.494\n",
      "[Iteration 170/29840] TRAIN loss: 5.455\n",
      "[Iteration 171/29840] TRAIN loss: 5.466\n",
      "[Iteration 172/29840] TRAIN loss: 5.480\n",
      "[Iteration 173/29840] TRAIN loss: 5.413\n",
      "[Iteration 174/29840] TRAIN loss: 5.428\n",
      "[Iteration 175/29840] TRAIN loss: 5.445\n",
      "[Iteration 176/29840] TRAIN loss: 5.464\n",
      "[Iteration 177/29840] TRAIN loss: 5.422\n",
      "[Iteration 178/29840] TRAIN loss: 5.524\n",
      "[Iteration 179/29840] TRAIN loss: 5.414\n",
      "[Iteration 180/29840] TRAIN loss: 5.461\n",
      "[Iteration 181/29840] TRAIN loss: 5.450\n",
      "[Iteration 182/29840] TRAIN loss: 5.501\n",
      "[Iteration 183/29840] TRAIN loss: 5.492\n",
      "[Iteration 184/29840] TRAIN loss: 5.471\n",
      "[Iteration 185/29840] TRAIN loss: 5.478\n",
      "[Iteration 186/29840] TRAIN loss: 5.476\n",
      "[Iteration 187/29840] TRAIN loss: 5.453\n",
      "[Iteration 188/29840] TRAIN loss: 5.475\n",
      "[Iteration 189/29840] TRAIN loss: 5.498\n",
      "[Iteration 190/29840] TRAIN loss: 5.477\n",
      "[Iteration 191/29840] TRAIN loss: 5.436\n",
      "[Iteration 192/29840] TRAIN loss: 5.491\n",
      "[Iteration 193/29840] TRAIN loss: 5.492\n",
      "[Iteration 194/29840] TRAIN loss: 5.509\n",
      "[Iteration 195/29840] TRAIN loss: 5.416\n",
      "[Iteration 196/29840] TRAIN loss: 5.448\n",
      "[Iteration 197/29840] TRAIN loss: 5.480\n",
      "[Iteration 198/29840] TRAIN loss: 5.428\n",
      "[Iteration 199/29840] TRAIN loss: 5.479\n",
      "[Iteration 200/29840] TRAIN loss: 5.429\n",
      "[Iteration 201/29840] TRAIN loss: 5.477\n",
      "[Iteration 202/29840] TRAIN loss: 5.442\n",
      "[Iteration 203/29840] TRAIN loss: 5.442\n",
      "[Iteration 204/29840] TRAIN loss: 5.446\n",
      "[Iteration 205/29840] TRAIN loss: 5.457\n",
      "[Iteration 206/29840] TRAIN loss: 5.385\n",
      "[Iteration 207/29840] TRAIN loss: 5.424\n",
      "[Iteration 208/29840] TRAIN loss: 5.492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 209/29840] TRAIN loss: 5.476\n",
      "[Iteration 210/29840] TRAIN loss: 5.454\n",
      "[Iteration 211/29840] TRAIN loss: 5.454\n",
      "[Iteration 212/29840] TRAIN loss: 5.437\n",
      "[Iteration 213/29840] TRAIN loss: 5.513\n",
      "[Iteration 214/29840] TRAIN loss: 5.328\n",
      "[Iteration 215/29840] TRAIN loss: 5.510\n",
      "[Iteration 216/29840] TRAIN loss: 5.482\n",
      "[Iteration 217/29840] TRAIN loss: 5.467\n",
      "[Iteration 218/29840] TRAIN loss: 5.432\n",
      "[Iteration 219/29840] TRAIN loss: 5.433\n",
      "[Iteration 220/29840] TRAIN loss: 5.480\n",
      "[Iteration 221/29840] TRAIN loss: 5.397\n",
      "[Iteration 222/29840] TRAIN loss: 5.465\n",
      "[Iteration 223/29840] TRAIN loss: 5.488\n",
      "[Iteration 224/29840] TRAIN loss: 5.449\n",
      "[Iteration 225/29840] TRAIN loss: 5.472\n",
      "[Iteration 226/29840] TRAIN loss: 5.458\n",
      "[Iteration 227/29840] TRAIN loss: 5.465\n",
      "[Iteration 228/29840] TRAIN loss: 5.454\n",
      "[Iteration 229/29840] TRAIN loss: 5.429\n",
      "[Iteration 230/29840] TRAIN loss: 5.440\n",
      "[Iteration 231/29840] TRAIN loss: 5.479\n",
      "[Iteration 232/29840] TRAIN loss: 5.377\n",
      "[Iteration 233/29840] TRAIN loss: 5.485\n",
      "[Iteration 234/29840] TRAIN loss: 5.393\n",
      "[Iteration 235/29840] TRAIN loss: 5.426\n",
      "[Iteration 236/29840] TRAIN loss: 5.472\n",
      "[Iteration 237/29840] TRAIN loss: 5.474\n",
      "[Iteration 238/29840] TRAIN loss: 5.472\n",
      "[Iteration 239/29840] TRAIN loss: 5.500\n",
      "[Iteration 240/29840] TRAIN loss: 5.420\n",
      "[Iteration 241/29840] TRAIN loss: 5.372\n",
      "[Iteration 242/29840] TRAIN loss: 5.443\n",
      "[Iteration 243/29840] TRAIN loss: 5.460\n",
      "[Iteration 244/29840] TRAIN loss: 5.438\n",
      "[Iteration 245/29840] TRAIN loss: 5.443\n",
      "[Iteration 246/29840] TRAIN loss: 5.462\n",
      "[Iteration 247/29840] TRAIN loss: 5.491\n",
      "[Iteration 248/29840] TRAIN loss: 5.371\n",
      "[Iteration 249/29840] TRAIN loss: 5.417\n",
      "[Iteration 250/29840] TRAIN loss: 5.464\n",
      "[Iteration 251/29840] TRAIN loss: 5.455\n",
      "[Iteration 252/29840] TRAIN loss: 5.411\n",
      "[Iteration 253/29840] TRAIN loss: 5.458\n",
      "[Iteration 254/29840] TRAIN loss: 5.489\n",
      "[Iteration 255/29840] TRAIN loss: 5.419\n",
      "[Iteration 256/29840] TRAIN loss: 5.403\n",
      "[Iteration 257/29840] TRAIN loss: 5.458\n",
      "[Iteration 258/29840] TRAIN loss: 5.479\n",
      "[Iteration 259/29840] TRAIN loss: 5.452\n",
      "[Iteration 260/29840] TRAIN loss: 5.455\n",
      "[Iteration 261/29840] TRAIN loss: 5.427\n",
      "[Iteration 262/29840] TRAIN loss: 5.473\n",
      "[Iteration 263/29840] TRAIN loss: 5.397\n",
      "[Iteration 264/29840] TRAIN loss: 5.402\n",
      "[Iteration 265/29840] TRAIN loss: 5.407\n",
      "[Iteration 266/29840] TRAIN loss: 5.497\n",
      "[Iteration 267/29840] TRAIN loss: 5.493\n",
      "[Iteration 268/29840] TRAIN loss: 5.358\n",
      "[Iteration 269/29840] TRAIN loss: 5.510\n",
      "[Iteration 270/29840] TRAIN loss: 5.387\n",
      "[Iteration 271/29840] TRAIN loss: 5.441\n",
      "[Iteration 272/29840] TRAIN loss: 5.368\n",
      "[Iteration 273/29840] TRAIN loss: 5.444\n",
      "[Iteration 274/29840] TRAIN loss: 5.456\n",
      "[Iteration 275/29840] TRAIN loss: 5.444\n",
      "[Iteration 276/29840] TRAIN loss: 5.493\n",
      "[Iteration 277/29840] TRAIN loss: 5.389\n",
      "[Iteration 278/29840] TRAIN loss: 5.454\n",
      "[Iteration 279/29840] TRAIN loss: 5.430\n",
      "[Iteration 280/29840] TRAIN loss: 5.439\n",
      "[Iteration 281/29840] TRAIN loss: 5.401\n",
      "[Iteration 282/29840] TRAIN loss: 5.415\n",
      "[Iteration 283/29840] TRAIN loss: 5.480\n",
      "[Iteration 284/29840] TRAIN loss: 5.371\n",
      "[Iteration 285/29840] TRAIN loss: 5.397\n",
      "[Iteration 286/29840] TRAIN loss: 5.467\n",
      "[Iteration 287/29840] TRAIN loss: 5.451\n",
      "[Iteration 288/29840] TRAIN loss: 5.438\n",
      "[Iteration 289/29840] TRAIN loss: 5.346\n",
      "[Iteration 290/29840] TRAIN loss: 5.456\n",
      "[Iteration 291/29840] TRAIN loss: 5.433\n",
      "[Iteration 292/29840] TRAIN loss: 5.403\n",
      "[Iteration 293/29840] TRAIN loss: 5.450\n",
      "[Iteration 294/29840] TRAIN loss: 5.454\n",
      "[Iteration 295/29840] TRAIN loss: 5.496\n",
      "[Iteration 296/29840] TRAIN loss: 5.439\n",
      "[Iteration 297/29840] TRAIN loss: 5.436\n",
      "[Iteration 298/29840] TRAIN loss: 5.358\n",
      "[Iteration 299/29840] TRAIN loss: 5.459\n",
      "[Iteration 300/29840] TRAIN loss: 5.455\n",
      "[Iteration 301/29840] TRAIN loss: 5.322\n",
      "[Iteration 302/29840] TRAIN loss: 5.496\n",
      "[Iteration 303/29840] TRAIN loss: 5.424\n",
      "[Iteration 304/29840] TRAIN loss: 5.467\n",
      "[Iteration 305/29840] TRAIN loss: 5.411\n",
      "[Iteration 306/29840] TRAIN loss: 5.424\n",
      "[Iteration 307/29840] TRAIN loss: 5.483\n",
      "[Iteration 308/29840] TRAIN loss: 5.449\n",
      "[Iteration 309/29840] TRAIN loss: 5.456\n",
      "[Iteration 310/29840] TRAIN loss: 5.462\n",
      "[Iteration 311/29840] TRAIN loss: 5.416\n",
      "[Iteration 312/29840] TRAIN loss: 5.417\n",
      "[Iteration 313/29840] TRAIN loss: 5.494\n",
      "[Iteration 314/29840] TRAIN loss: 5.438\n",
      "[Iteration 315/29840] TRAIN loss: 5.463\n",
      "[Iteration 316/29840] TRAIN loss: 5.325\n",
      "[Iteration 317/29840] TRAIN loss: 5.435\n",
      "[Iteration 318/29840] TRAIN loss: 5.420\n",
      "[Iteration 319/29840] TRAIN loss: 5.484\n",
      "[Iteration 320/29840] TRAIN loss: 5.417\n",
      "[Iteration 321/29840] TRAIN loss: 5.432\n",
      "[Iteration 322/29840] TRAIN loss: 5.435\n",
      "[Iteration 323/29840] TRAIN loss: 5.430\n",
      "[Iteration 324/29840] TRAIN loss: 5.465\n",
      "[Iteration 325/29840] TRAIN loss: 5.373\n",
      "[Iteration 326/29840] TRAIN loss: 5.402\n",
      "[Iteration 327/29840] TRAIN loss: 5.428\n",
      "[Iteration 328/29840] TRAIN loss: 5.453\n",
      "[Iteration 329/29840] TRAIN loss: 5.428\n",
      "[Iteration 330/29840] TRAIN loss: 5.402\n",
      "[Iteration 331/29840] TRAIN loss: 5.378\n",
      "[Iteration 332/29840] TRAIN loss: 5.451\n",
      "[Iteration 333/29840] TRAIN loss: 5.404\n",
      "[Iteration 334/29840] TRAIN loss: 5.445\n",
      "[Iteration 335/29840] TRAIN loss: 5.440\n",
      "[Iteration 336/29840] TRAIN loss: 5.442\n",
      "[Iteration 337/29840] TRAIN loss: 5.460\n",
      "[Iteration 338/29840] TRAIN loss: 5.414\n",
      "[Iteration 339/29840] TRAIN loss: 5.419\n",
      "[Iteration 340/29840] TRAIN loss: 5.466\n",
      "[Iteration 341/29840] TRAIN loss: 5.461\n",
      "[Iteration 342/29840] TRAIN loss: 5.457\n",
      "[Iteration 343/29840] TRAIN loss: 5.363\n",
      "[Iteration 344/29840] TRAIN loss: 5.351\n",
      "[Iteration 345/29840] TRAIN loss: 5.447\n",
      "[Iteration 346/29840] TRAIN loss: 5.434\n",
      "[Iteration 347/29840] TRAIN loss: 5.419\n",
      "[Iteration 348/29840] TRAIN loss: 5.470\n",
      "[Iteration 349/29840] TRAIN loss: 5.490\n",
      "[Iteration 350/29840] TRAIN loss: 5.451\n",
      "[Iteration 351/29840] TRAIN loss: 5.469\n",
      "[Iteration 352/29840] TRAIN loss: 5.472\n",
      "[Iteration 353/29840] TRAIN loss: 5.400\n",
      "[Iteration 354/29840] TRAIN loss: 5.444\n",
      "[Iteration 355/29840] TRAIN loss: 5.473\n",
      "[Iteration 356/29840] TRAIN loss: 5.428\n",
      "[Iteration 357/29840] TRAIN loss: 5.411\n",
      "[Iteration 358/29840] TRAIN loss: 5.463\n",
      "[Iteration 359/29840] TRAIN loss: 5.396\n",
      "[Iteration 360/29840] TRAIN loss: 5.393\n",
      "[Iteration 361/29840] TRAIN loss: 5.482\n",
      "[Iteration 362/29840] TRAIN loss: 5.458\n",
      "[Iteration 363/29840] TRAIN loss: 5.376\n",
      "[Iteration 364/29840] TRAIN loss: 5.391\n",
      "[Iteration 365/29840] TRAIN loss: 5.446\n",
      "[Iteration 366/29840] TRAIN loss: 5.346\n",
      "[Iteration 367/29840] TRAIN loss: 5.271\n",
      "[Iteration 368/29840] TRAIN loss: 5.440\n",
      "[Iteration 369/29840] TRAIN loss: 5.412\n",
      "[Iteration 370/29840] TRAIN loss: 5.451\n",
      "[Iteration 371/29840] TRAIN loss: 5.368\n",
      "[Iteration 372/29840] TRAIN loss: 5.418\n",
      "[Iteration 373/29840] TRAIN loss: 5.437\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of type Variable[torch.cuda.LongTensor] but found type Variable[torch.cuda.FloatTensor] for argument #1 'other'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-719a66c8bed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 loss_func = torch.nn.CrossEntropyLoss(ignore_index = -1))\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_nth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/ConvAge/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, train_loader, val_loader, num_epochs, log_nth)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# Only allow images/pixels with label >= 0 e.g. for segmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mtargets_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtargets_mask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlog_nth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of type Variable[torch.cuda.LongTensor] but found type Variable[torch.cuda.FloatTensor] for argument #1 'other'"
     ]
    }
   ],
   "source": [
    "from classifiers.segmentation_nn import SegmentationNN\n",
    "from solver import Solver\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=4,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                         batch_size=4,\n",
    "                                         shuffle=False,\n",
    "                                         num_workers=4)\n",
    "model = SegmentationNN()\n",
    "    \n",
    "\n",
    "solver = Solver(optim_args={\"lr\": 1e-4,\n",
    "                            \"eps\": 1e-8\n",
    "                            },\n",
    "                loss_func = torch.nn.CrossEntropyLoss(ignore_index = -1))\n",
    "\n",
    "solver.train(model, train_loader, val_loader, log_nth=1, num_epochs=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
